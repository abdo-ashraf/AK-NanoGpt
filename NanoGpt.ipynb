{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lQ0wFYZEk0y9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4k740-l_k0y-"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "block_size = 256\n",
        "dim_embd = 384\n",
        "num_heads = 6\n",
        "head_size = 384\n",
        "seed = 1337\n",
        "learning_rate = 3e-4\n",
        "ffrwd_in_features = 384\n",
        "\n",
        "g = torch.manual_seed(seed)\n",
        "max_iters = 2500\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "KiKWzqu8lBwA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yqwnykkk0y-",
        "outputId": "d07fc85f-df08-46fc-9311-99b239097e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(text) = 1115394 characters\n",
            "-------------Data-------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open('./input.txt') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"{len(text) = } characters\",\n",
        "       end='\\n-------------Data-------------\\n')\n",
        "\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biJPWhsSk0y_"
      },
      "source": [
        "### Build our char level vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79BeVu3Sk0zA",
        "outputId": "f4272b01-e472-4ec4-a678-ec96126ba8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "\n",
        "stoi = {ch:idx for idx, ch in enumerate(chars)}\n",
        "itos = {idx:ch for idx, ch in enumerate(chars)}\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUmfXZtbk0zA",
        "outputId": "484134ed-0943-47a4-8ce1-14de5d8c7032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47, 1, 50, 53, 60, 43, 1, 58, 46, 47, 57, 1, 45, 39, 51, 43, 8]\n",
            "i love this game.\n"
          ]
        }
      ],
      "source": [
        "encode = lambda text: [stoi[c] for c in text]\n",
        "decode = lambda tokens: ''.join([itos[c] for c in tokens])\n",
        "\n",
        "raw = 'i love this game.'\n",
        "tokens = encode(raw)\n",
        "print(tokens)\n",
        "print(decode(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF8NbOxyk0zA"
      },
      "source": [
        "### Encode entire data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs-umn8Sk0zA",
        "outputId": "e03028fb-1d3d-437b-fc0c-acf92e2286f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.LongTensor\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text)).long()\n",
        "print(data.shape, data.type())\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXHpkRBxk0zA"
      },
      "source": [
        "### Data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3W7wf0Nk0zA",
        "outputId": "4c54d038-89ea-49c8-c269-062f2fc88369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0])\n"
          ]
        }
      ],
      "source": [
        "train_data, val_data = train_test_split(data, shuffle=False, test_size=0.1)\n",
        "print(train_data[:block_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEaLnDynk0zB",
        "outputId": "94415f87-349d-442f-b833-859cac02bfb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 256])\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split_type, device):\n",
        "    data = train_data if split_type=='train' else val_data\n",
        "    ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,), generator=g)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train', device=device)\n",
        "print(xb.shape, yb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xW6fzm0uk0zB"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters, device):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visiIHo_k0zB"
      },
      "source": [
        "### Mathematical Trick to vectorized weighted aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2JzlB4KNk0zB"
      },
      "outputs": [],
      "source": [
        "B,T,C = (4,8,2)\n",
        "x = torch.randn(B,T,C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sglOiyhqk0zB"
      },
      "outputs": [],
      "source": [
        "## Version 1\n",
        "xbow = torch.zeros_like(x)\n",
        "\n",
        "for t in range(T):\n",
        "    x_prev = x[:, :t+1, :]\n",
        "    xbow[:,[t], :] = torch.mean(x_prev, dim=1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNxQT26Kk0zB",
        "outputId": "107aa5eb-611f-4a0f-b3b3-602065f69e1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "## Version 2\n",
        "wei = torch.tril(torch.ones((T,T))).float()\n",
        "wei = wei / wei.sum(dim=1, keepdim=True)\n",
        "xbow2 = wei @ x\n",
        "torch.allclose(xbow2, xbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRvOcqEhk0zB",
        "outputId": "ba2fd9cf-acc2-48fd-b249-6867045bdf8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "## Version 3: using softmax\n",
        "tril = torch.tril(torch.ones((T,T))).float()\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtooryOXk0zB"
      },
      "source": [
        "### Simple model with only one self attention (only one head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TAKDrdCCk0zB"
      },
      "outputs": [],
      "source": [
        "class SelfAttention_Head(nn.Module):\n",
        "    \"\"\"One head of self-attention\"\"\"\n",
        "    def __init__(self, input_features, head_features, block_size, dop=0.3):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.head_features = head_features\n",
        "        self.input_features = input_features\n",
        "\n",
        "        self.WQ = nn.Linear(input_features, head_features, bias=False)\n",
        "        self.WK = nn.Linear(input_features, head_features, bias=False)\n",
        "        self.WV = nn.Linear(input_features, head_features, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dop)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        T = x.shape[1]\n",
        "        q = self.WQ(x)\n",
        "        k = self.WK(x)\n",
        "\n",
        "        wei = q @ k.transpose(1,2) * self.head_features**-0.5\n",
        "        ## this mask to disconnect the token from following tokens in the seqeunce\n",
        "        ## we will use this mask during training only\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "        attention_weights = self.dropout(F.softmax(wei, dim=1)) ## this is attention weights\n",
        "\n",
        "        v = self.WV(x)\n",
        "        out = attention_weights @ v\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0c8IGmvKk0zC"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
        "    def __init__(self, num_heads, input_features, head_features, block_size, dop=0.3):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttention_Head(input_features, head_features, block_size) for _ in range(num_heads)])\n",
        "        ## Projection to make output compatible with residual adding\n",
        "        self.proj = nn.Linear(head_features*num_heads, input_features)\n",
        "        self.dropout = nn.Dropout(dop)\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "V-yS-y26k0zC"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, in_features, dop=0.3):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, in_features * 4),\n",
        "            nn.ReLU(),\n",
        "            ## Projection to make output compatible with residual adding\n",
        "            nn.Linear(in_features * 4, in_features),\n",
        "            nn.Dropout(dop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I-QbWT-Xk0zC"
      },
      "outputs": [],
      "source": [
        "## LayerNorm: same as BatchNorm ,but it normalize the rows not the columns\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, num_heads, input_features, head_features, block_size):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(num_heads, input_features, head_features//num_heads, block_size)\n",
        "        self.ffwd = FeedForward(head_features)\n",
        "        self.ln1 = nn.LayerNorm(input_features)\n",
        "        self.ln2 = nn.LayerNorm(head_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## (x +) is for residual connections\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0-AjQi8nk0zC"
      },
      "outputs": [],
      "source": [
        "class Attentioned_LM(nn.Module):\n",
        "    def __init__(self, vocab_size, dim_embd, block_size, head_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.embd_layers = nn.Embedding(vocab_size, dim_embd)\n",
        "        self.position_encoding_layer = nn.Embedding(block_size, dim_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(num_heads, dim_embd, head_size, block_size),\n",
        "            Block(num_heads, dim_embd, head_size, block_size),\n",
        "            Block(num_heads, dim_embd, head_size, block_size),\n",
        "            Block(num_heads, dim_embd, head_size, block_size),\n",
        "            Block(num_heads, dim_embd, head_size, block_size),\n",
        "            Block(num_heads, dim_embd, head_size, block_size),\n",
        "            nn.LayerNorm(head_size)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(head_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        device = idx.device\n",
        "\n",
        "        embdings = self.embd_layers(idx)\n",
        "        pos_encodes = self.position_encoding_layer(torch.arange(T, device=device))\n",
        "        x = embdings + pos_encodes ## (B,T,dim_embd) + (T,dim_embd) = (B,T,dim_embd) broadcast happened\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batch, seq_len, embds = logits.shape\n",
        "            logits = logits.view(batch*seq_len, embds)\n",
        "            targets = targets.view(batch*seq_len)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop input tokens for the length of block_size, so we able to positional encode them\n",
        "            idx_context = idx[:, -self.block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_context)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "## In this code we still preserve history tokens, however we don't use them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTsNeOqFk0zC",
        "outputId": "aacecb86-d2a2-41e9-a5ac-51578287fc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16384, 65])\n",
            "tensor(4.3529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "rXvLFCUS majJr?b,t.HfxNDzd,HEqGfsA3Wj gE,qVEX\n",
            "nO,BepG AQj!\n",
            "it3xfs?',ycT,HRitA!dGd?CzOqmVjMyFwLOu&LVg\n"
          ]
        }
      ],
      "source": [
        "m = Attentioned_LM(vocab_size, dim_embd, block_size, head_size, num_heads).to(device)\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "input = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
        "print(decode(m.generate(input, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "optimizer = AdamW(m.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRx_Ww33k0zC",
        "outputId": "fa6b5d23-27f2-471f-ef07-dfa36330c3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3530, val loss 4.3490\n",
            "step 500: train loss 0.3217, val loss 0.3787\n",
            "step 1000: train loss 0.0449, val loss 0.0694\n",
            "step 1500: train loss 0.0195, val loss 0.0341\n",
            "step 2000: train loss 0.0138, val loss 0.0256\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss(m, eval_iters, device=device)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', device=device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    m.eval()\n",
        "    input = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
        "    print(decode(m.generate(input, max_new_tokens=1000)[0].tolist()))\n",
        "    m.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TENycEluFAe",
        "outputId": "8cdfb2c7-1315-427a-b04b-ee001b3d692f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " pppppprrrrrrrrRuiiiiiiiiiiiiiaiiniiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiittesssssssssstcrrrrrrnnnnnnnnnnnnntttteeeeeeeeeeeeeeeeeeeeeee\n",
            " ssslllllllllrrrcchrrssssssssssssssssmmmmmmmmmmmmmmmmmmmmmymmmmmsssssssssssssssssssllfeintan'e, nsntNN\n",
            "Gno pere the\n",
            "un tac notne ther, ver torth dyoucsor:\n",
            " leg it thant dourshalt salit leand, vadeig tpith mfraf,\n",
            "soer dnut samus. sne.\n",
            "E'W\n",
            "sad yereicje,\n",
            "Whin dressnmer, maner hint hef it, donn, itly plifly paly jago,\n",
            "Ali, ye,\n",
            "nY eWhan w won trokontot fite meirouonuncist\n",
            "a shic uthe .\n",
            "n Youn blelves,platred, syoks out hon haneremant!\n",
            "UUute forra,\n",
            "IN aswer\n",
            "Anew y lo to to th mounint yoet:\n",
            "I, sandirson foie lhslm seneses oufrece loiand mise.\n",
            "SThmeNAnisf! te the ohint ughy to thumeerTlel ande thruces tot wosle th,\n",
            "Khener ic rin.\n",
            "FOUE:\n",
            "Parto, threw youssocooud fouskins aren,\n",
            "Wh\n",
            "Theunse smear woldows, fag may alrolk thoe ravel wod yt toakounny\n",
            "MOrwe torpo sou hpistrefptre, me lemnerlev eaned hhevnengres axod poroce an tirs.\n",
            "\n",
            "MARDiss an nhobne an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(m, '/content/nanoGpt.pt')"
      ],
      "metadata": {
        "id": "NtTT8Lpo2gvJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "U3xAuriik0zC"
      },
      "outputs": [],
      "source": [
        "# GG"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}