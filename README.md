# AK-NanoGpt
In this repository, I am implementing Andrej Karpathy's NanoGPT, inspired by the "Attention is All You Need" paper. This implementation focuses on creating a lightweight, minimalistic version of a Transformer-based language model. It serves as both a learning exercise and a practical demonstration of the core principles of modern deep learning models.

The repository aims to simplify the key concepts behind Transformers while maintaining a functional and efficient implementation of self-attention, multi-head attention, and positional encodings as described in the original paper.
